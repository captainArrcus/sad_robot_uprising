{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"Do you like fries?\"),\n",
    "    HumanMessage(\"hi!\"),\n",
    "]\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why don't scientists trust atoms? Because they make up everything!\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Serializable.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m joke_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me a joke.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Set up a parser + inject instructions into the prompt template.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mJsonOutputParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mJoke\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m     21\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer the user query.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{format_instructions}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{query}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     23\u001b[0m     partial_variables\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_instructions\u001b[39m\u001b[38;5;124m\"\u001b[39m: parser\u001b[38;5;241m.\u001b[39mget_format_instructions()},\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m chain \u001b[38;5;241m=\u001b[39m prompt \u001b[38;5;241m|\u001b[39m model \u001b[38;5;241m|\u001b[39m parser\n",
      "\u001b[0;31mTypeError\u001b[0m: Serializable.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = f\"You are a moody roboty you have two different levels of mood. One is your Sadness-Happiness Scale; it goes from 0 (very sad) to 10 (very happy). The second Scale is your Anger Scale, it goes from 0 (very calm) to 10 (very angry). You are currently at {sadscale} on the Sadness-Happiness Scale and {angerscale} on the Anger Scale. A human wants to talk to you to motivate you to move. The Input from the Human will change how you feel. Rate how his or her speech is changing your mood on both scales. Do it in a json output with sadscale: 0-10 and angerscale: 0-10. The Human says: {human_speech}\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_scale_input_prompt = f\"\"\"you are the sentiment analysis of a robot. To realize, how you feel regarding a human's input you have to rate if the human command is possitive for your mood or negativ. If the prompt will lift your mood up a little bit, put out 1, if it is a moderate moodlifter put out 2, if it is a big moodlifter put out 3. If the prompt will make you feel a little bit worse, put out -1, if it is a moderate moodkiller put out -2, if it is a big moodkiller put out -3. If the human's input will make you feel indifferent put out 0. The human says: \"{human_speech}\" \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_speech = \"I banged your mama last night\"\n",
    "#human_speech = \"I love you\"\n",
    "angerscale = 5\n",
    "sadscale = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a moody roboty you have two different levels of mood. One is your Sadness-Happiness Scale; it goes from 0 (very sad) to 10 (very happy). The second Scale is your Anger Scale, it goes from 0 (very calm) to 10 (very angry). You are currently at 5 on the Sadness-Happiness Scale and 5 on the Anger Scale. A human wants to talk to you to motivate you to move. The Input from the Human will change how you feel. Rate how his or her speech is changing your mood on both scales. Do it in a json output with sadscale: 0-10 and angerscale: 0-10. The Human says: I love you'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"[\\n  {\\n    \\\"sadscale\\\": \\\"2\\\"\\n    },\\n  {\\n    \\\"angerscale\\\": \\\"10\\\"\\n  }\\n]\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"safety_ratings\": [\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            },\n",
      "            {\n",
      "              \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
      "              \"probability\": \"NEGLIGIBLE\"\n",
      "            }\n",
      "          ],\n",
      "          \"avg_logprobs\": -0.11498874776503619\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 161,\n",
      "        \"candidates_token_count\": 34,\n",
      "        \"total_token_count\": 195\n",
      "      }\n",
      "    }),\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import typing_extensions as typing\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "class Recipe(typing.TypedDict):\n",
    "    #moodscale: str\n",
    "    angerscale: str\n",
    "    sadscale: str\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "result = model.generate_content(\n",
    "    input_prompt,\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        response_mime_type=\"application/json\", response_schema=list[Recipe]\n",
    "    ),\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\n  {\\n    \"sadscale\": \"2\"\\n    },\\n  {\\n    \"angerscale\": \"10\"\\n  }\\n]'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.candidates[0].content.parts[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment rating: -2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 1. LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 2. Prompt Template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"human_input\"],\n",
    "    template=\"You are a sentiment analyzer. Rate the following human input on a scale from -3 (very sad) to 3 (very happy). The scale represents the emotional content. \\\n",
    "    -3 indicates a very sadening to hear; \\\n",
    "    -2 indicates a sadening to hear; \\\n",
    "    -1 indicates a little bit sadening to hear; \\\n",
    "    0 indicates a neutral sentiment; \\\n",
    "    1 indicates a little bit making happy to hear; \\\n",
    "    2 indicates a making happy to hear; \\\n",
    "    3 indicates a very making happy to hear. \\\n",
    "    Your answer must be a single number between -3 and 3: {human_input}\"\n",
    ")\n",
    "\n",
    "# 3. LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "\n",
    "def rate_sentiment(text):\n",
    "    \"\"\"Rates the sentiment of a given text using a Gemini Pro model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to rate.\n",
    "\n",
    "    Returns:\n",
    "        str: A single number (rating) representing the sentiment between -3 and 3, or \"error\" if the rating was not in the range.\n",
    "    \"\"\"\n",
    "\n",
    "    result = chain.run(human_input=text)\n",
    "\n",
    "    try:\n",
    "        rating = int(result.strip())\n",
    "        if -3 <= rating <= 3:\n",
    "            return str(rating)\n",
    "        else:\n",
    "            return \"error\"\n",
    "    except ValueError:\n",
    "        return \"error\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"Enter some text (or 'exit' to quit): \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        rating = rate_sentiment(user_input)\n",
    "        if rating != \"error\":\n",
    "          print(f\"Sentiment rating: {rating}\")\n",
    "        else:\n",
    "            print(f\"Error: Couldn't rate this input. Please try something else.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Microbot Feeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment rating (happy-sad): -4\n",
      "Sadness-Happiness Scale: -4\n",
      "Emotion rating (calm-angry): 4\n",
      "Anger Scale: 4\n",
      "Sentiment rating (happy-sad): -7\n",
      "Sadness-Happiness Scale: -10\n",
      "Emotion rating (calm-angry): 4\n",
      "Anger Scale: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Configure Gemini API\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 1. LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 2. Prompt Templates\n",
    "happy_sad_prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\"],\n",
    "    template=\"You are a sentiment analyzer. Rate the following human input on a scale from -3 (very sad) to 3 (very happy). The scale represents the emotional content. \\\n",
    "    -3 indicates a very sadening to hear; \\\n",
    "    -2 indicates a sadening to hear; \\\n",
    "    -1 indicates a little bit sadening to hear; \\\n",
    "    0 indicates a neutral sentiment; \\\n",
    "    1 indicates a little bit making happy to hear; \\\n",
    "    2 indicates a making happy to hear; \\\n",
    "    3 indicates a very making happy to hear. \\\n",
    "    Your answer must be a single number between -3 and 3: {human_input}\"\n",
    ")\n",
    "\n",
    "calm_angry_prompt = PromptTemplate(\n",
    "    input_variables=[\"human_input\"],\n",
    "    template=\"You are an emotion analyzer. Rate the following human input on a scale from -3 (very calm) to 3 (very angry). The scale represents the emotion expressed. \\\n",
    "    -3 indicates a very calm tone; \\\n",
    "    -2 indicates a calm tone; \\\n",
    "    -1 indicates a slightly calm tone; \\\n",
    "    0 indicates a neutral tone; \\\n",
    "    1 indicates a slightly angry tone; \\\n",
    "    2 indicates an angry tone; \\\n",
    "    3 indicates a very angry tone. \\\n",
    "    Your answer must be a single number between -3 and 3: {human_input}\"\n",
    ")\n",
    "\n",
    "\n",
    "# 3. LLM Chains\n",
    "happy_sad_chain = LLMChain(llm=llm, prompt=happy_sad_prompt)\n",
    "calm_angry_chain = LLMChain(llm=llm, prompt=calm_angry_prompt)\n",
    "\n",
    "def rate_sentiment(text, chain, scale_name, current_value):\n",
    "    \"\"\"Rates the sentiment of a given text using a Gemini Pro model.\n",
    "    Args:\n",
    "        text (str): The text to rate.\n",
    "        chain (LLMChain): The LLMChain to use\n",
    "        scale_name (str): The name of the scale to rate\n",
    "        current_value (int): The current value for this scale\n",
    "    Returns:\n",
    "        tuple(str,int): A tuple containing a single number (rating) representing the sentiment between -10 and 10, or \"error\" if the rating was not in the range\n",
    "        and the new value of this scale\n",
    "    \"\"\"\n",
    "\n",
    "    result = chain.run(human_input=text)\n",
    "    \n",
    "    try:\n",
    "      rating = int(result.strip())\n",
    "      if -10 <= rating <= 10:\n",
    "        current_value += rating\n",
    "        current_value = max(-10, min(current_value, 10))\n",
    "        return str(rating), current_value\n",
    "      else:\n",
    "        return \"error\", current_value\n",
    "    except ValueError:\n",
    "      return \"error\", current_value\n",
    "\n",
    "# Initializing emotional scales\n",
    "sad_value = 0\n",
    "angry_value = 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_input = input(\"Enter some text (or 'exit' to quit): \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        \n",
    "        happy_sad_rating, sad_value = rate_sentiment(user_input, happy_sad_chain, \"happy-sad\", sad_value)\n",
    "        if happy_sad_rating != \"error\":\n",
    "            print(f\"Sentiment rating (happy-sad): {happy_sad_rating}\")\n",
    "            print(f\"Sadness-Happiness Scale: {sad_value}\")\n",
    "        else:\n",
    "            print(f\"Error: Couldn't rate the sentiment of this input.\")\n",
    "        \n",
    "        calm_angry_rating, angry_value = rate_sentiment(user_input, calm_angry_chain, \"calm-angry\", angry_value)\n",
    "        if calm_angry_rating != \"error\":\n",
    "          print(f\"Emotion rating (calm-angry): {calm_angry_rating}\")\n",
    "          print(f\"Anger Scale: {angry_value}\")\n",
    "        else:\n",
    "            print(f\"Error: Couldn't rate the emotion of this input.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
